{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995f2a44",
   "metadata": {},
   "source": [
    "# Biotech News and Trends Concierge Agent\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Problem\n",
    "\n",
    "Biotechnology moves quickly, and meaningful developments are scattered across dozens of news sources, journals, and industry feeds. Manually tracking these updates is time-consuming, inconsistent, and prone to missing important signals. Raw article text is noisy and difficult to compare, making it hard to identify which topics are emerging, which are declining, and where industry attention is shifting. There is no simple, automated way to transform daily biotech news into structured, trend-level insights.\n",
    "\n",
    "### Solution/Objective\n",
    "\n",
    "This project implements an automated RSS-driven pipeline that collects biotech articles, summarizes them using an LLM, and extracts key concepts for trend analysis. A Trend Agent clusters related topics, measures their frequency and momentum, and highlights emerging or unusual patterns across the dataset. The final output is a structured, data-driven trend report that makes it easy to monitor the biotech landscape, spot early signals, and stay informed without manual curation.\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b26eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (6.0.12)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from feedparser) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.52.0-py3-none-any.whl.metadata (46 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-genai)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from google-genai)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from google-genai) (2.32.2)\n",
      "Collecting tenacity<9.2.0,>=8.2.3 (from google-genai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.6.0)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.9.0->google-genai)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.11.0 (from google-genai)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.9.0->google-genai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jeelf\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.4.8)\n",
      "Downloading google_genai-1.52.0-py3-none-any.whl (261 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 12.5 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: websockets, typing-extensions, tenacity, rsa, typing-inspection, pydantic-core, google-auth, anyio, pydantic, httpx, google-genai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.2.0\n",
      "    Uninstalling anyio-4.2.0:\n",
      "      Successfully uninstalled anyio-4.2.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.13\n",
      "    Uninstalling pydantic-1.10.13:\n",
      "      Successfully uninstalled pydantic-1.10.13\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "Successfully installed anyio-4.11.0 google-auth-2.43.0 google-genai-1.52.0 httpx-0.28.1 pydantic-2.12.4 pydantic-core-2.41.5 rsa-4.9.1 tenacity-9.1.2 typing-extensions-4.15.0 typing-inspection-0.4.2 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser\n",
    "!pip install google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2edd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from google import genai\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230af5f",
   "metadata": {},
   "source": [
    "## Fetch RSS Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f65b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class RSSFetcher:\n",
    "    def __init__(self, config_path=\"config/rss_feeds.json\", storage_path=\"../data/rss_raw.json\"):\n",
    "        self.config_path = Path(config_path)\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load feeds from config file\n",
    "        with open(self.config_path, \"r\") as f:\n",
    "            self.rss_urls = json.load(f)[\"feeds\"]\n",
    "\n",
    "    def _infer_source(self, url: str) -> str:\n",
    "        \"\"\"Infer source name from URL.\"\"\"\n",
    "        if \"fiercebiotech.com/rss/biotech\" in url or \"fiercebiotech.com/rss/xml\" in url:\n",
    "            return \"FierceBiotech\"\n",
    "        elif \"labiotech.eu\" in url:\n",
    "            return \"Labiotech.eu\"\n",
    "        elif \"GenEngNews\" in url or \"genengnews.com\" in url:\n",
    "            return \"GEN (Genetic Engineering & Biotech News)\"\n",
    "        elif \"sciencedaily.com\" in url and \"genetics_gene_therapy\" in url:\n",
    "            return \"ScienceDaily – Gene Therapy\"\n",
    "        elif \"bioworld.com/rss/topic/10\" in url:\n",
    "            return \"BioWorld Omics / Genomics\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    def fetch(self):\n",
    "        \"\"\"Fetch articles from all RSS URLs.\"\"\"\n",
    "        all_articles = []\n",
    "\n",
    "        for url in self.rss_urls:\n",
    "            feed = feedparser.parse(url)\n",
    "            source = self._infer_source(url)\n",
    "\n",
    "            print(f\"Fetching from {source}: {url}\")\n",
    "\n",
    "            for entry in feed.entries:\n",
    "                article = {\n",
    "                    \"title\": entry.get(\"title\"),\n",
    "                    \"summary\": entry.get(\"summary\", \"\"),\n",
    "                    \"link\": entry.get(\"link\"),\n",
    "                    \"published\": entry.get(\"published\") or entry.get(\"updated\") or None,\n",
    "                    \"source\": source,\n",
    "                    \"fetched_at\": datetime.datetime.utcnow().isoformat()\n",
    "                }\n",
    "                all_articles.append(article)\n",
    "\n",
    "        self._save(all_articles)\n",
    "        return all_articles\n",
    "\n",
    "    def _save(self, articles):   # might not be needed\n",
    "        \"\"\"Save raw fetched articles.\"\"\"\n",
    "        with open(self.storage_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(articles, f, indent=2)\n",
    "\n",
    "        print(f\"Saved {len(articles)} articles to {self.storage_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07df2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from FierceBiotech: https://www.fiercebiotech.com/rss/biotech/xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeelf\\AppData\\Local\\Temp\\ipykernel_30316\\3052094806.py:48: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"fetched_at\": datetime.datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from Labiotech.eu: https://www.labiotech.eu/feed/\n",
      "Fetching from GEN (Genetic Engineering & Biotech News): https://feeds.feedburner.com/GenEngNews\n",
      "Fetching from ScienceDaily – Gene Therapy: https://rss.sciencedaily.com/genetics_gene_therapy.xml\n",
      "Fetching from BioWorld Omics / Genomics: https://www.bioworld.com/rss/topic/10\n",
      "Saved 37 articles to ..\\data\\rss_raw.json\n",
      "Fetched 37 articles.\n",
      "                                               title  \\\n",
      "0  <a href=\"https://www.fiercebiotech.com/biotech...   \n",
      "1  <a href=\"https://www.fiercebiotech.com/biotech...   \n",
      "2  <a href=\"https://www.fiercebiotech.com/biotech...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  Hundreds of industry leaders have signed a let...   \n",
      "1  The FDA is hiring more than 1,000 new employee...   \n",
      "2  Gilead’s general counsel and EVP of corporate ...   \n",
      "\n",
      "                                                link             published  \\\n",
      "0  https://www.fiercebiotech.com/biotech/letter-m...   Nov 21, 2025 4:16pm   \n",
      "1  https://www.fiercebiotech.com/biotech/fda-kick...  Nov 21, 2025 11:20am   \n",
      "2  https://www.fiercebiotech.com/biotech/chutes-l...   Nov 20, 2025 4:26pm   \n",
      "\n",
      "          source                  fetched_at  \n",
      "0  FierceBiotech  2025-11-23T02:17:14.661368  \n",
      "1  FierceBiotech  2025-11-23T02:17:14.661368  \n",
      "2  FierceBiotech  2025-11-23T02:17:14.661368  \n"
     ]
    }
   ],
   "source": [
    "# Run Fetcher for generic RSS (including PubMed)\n",
    "#from src.fetcher import RSSFetcher\n",
    "\n",
    "# This will load feeds from config/rss_feeds.json by default\n",
    "fetcher = RSSFetcher()\n",
    "\n",
    "articles = fetcher.fetch()\n",
    "\n",
    "# Convert to df\n",
    "articles = pd.DataFrame(articles)\n",
    "\n",
    "print(f\"Fetched {len(articles)} articles.\")\n",
    "print(articles[:3])  # show first 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afba357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: <a href=\"https://www.fiercebiotech.com/biotech/letter-makary-biotech-ceos-push-fda-stability-and-say-volatility-threatens-us-innovation\" hreflang=\"en\">In letter to Makary, biotech CEOs push for FDA stability and say volatility threatens US innovation</a>\n",
      "Title: <a href=\"https://www.fiercebiotech.com/biotech/fda-kicks-hiring-spree-and-new-communication-program-speed-sluggish-drug-reviews\" hreflang=\"en\"> FDA says it's hiring more than 1,000 new staffers, launches new comms program for review process</a>\n",
      "Title: <a href=\"https://www.fiercebiotech.com/biotech/chutes-ladders-gilead-abruptly-parts-ways-general-counsel\" hreflang=\"en\">Chutes &amp; Ladders—Gilead abruptly parts ways with general counsel</a>\n",
      "Title: <a href=\"https://www.fiercebiotech.com/biotech/fierce-biotech-layoff-tracker-2025\" hreflang=\"en\">Fierce Biotech Layoff Tracker 2025: Applied Tx lays off 46% of staff; Ensoma makes cuts</a>\n",
      "Title: <a href=\"https://www.fiercebiotech.com/biotech/nurix-trims-workforce-pivotal-trial-lead-btk-degrader-kicks\" hreflang=\"en\">Nurix trims workforce as pivotal trial for lead BTK degrader kicks off</a>\n",
      "\n",
      "Summary text: Hundreds of industry leaders have signed a letter to FDA Director Marty Makary, M.D., about the importance of having a predictable regulatory agency, citing a survey in which 82% of biopharma respondents said they were concerned about the FDA’s ability to function.\n",
      "Summary text: The FDA is hiring more than 1,000 new employees and rolling out a pilot communication program in an effort to speed up the review of new drugs.\n",
      "Summary text: Gilead’s general counsel and EVP of corporate affairs Deborah Telman will hang up the gloves by Dec. 5, the company announced in a brief filing Nov. 15. Gilead didn’t offer any explanation for Telman’s departure, plainly stating that the executive’s employment will “terminate later in the month.”\n",
      "Summary text: As always, if you know of layoffs occurring at a biotech, please reach out to the Fierce Biotech editorial team.\n",
      "Summary text: Nurix Therapeutics is trimming its workforce not long after launching a pivotal phase 2 trial for its lead cancer asset.\n"
     ]
    }
   ],
   "source": [
    "# Clean up articles and extract titles and summaries\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Convert summaries to plain text\n",
    "articles['summary_text'] = articles['summary'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "\n",
    "# Print just the top 5 titles and converted summaries\n",
    "for title in articles['title'].head(5):\n",
    "    print(\"Title:\", title)\n",
    "print()\n",
    "for summary_text in articles['summary_text'].head(5):\n",
    "    print(\"Summary text:\", summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d2993",
   "metadata": {},
   "source": [
    "## Summarize Articles (Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5ee896",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m api_key_env \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# from local environment variable, or can use .env file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39mapi_key_env)\n\u001b[0;32m      8\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m THROTTLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\google\\genai\\client.py:271\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[0;32m    268\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     http_options \u001b[38;5;241m=\u001b[39m HttpOptions(base_url\u001b[38;5;241m=\u001b[39mbase_url)\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_api_client(\n\u001b[0;32m    272\u001b[0m     vertexai\u001b[38;5;241m=\u001b[39mvertexai,\n\u001b[0;32m    273\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    274\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    275\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[0;32m    276\u001b[0m     location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[0;32m    277\u001b[0m     debug_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug_config,\n\u001b[0;32m    278\u001b[0m     http_options\u001b[38;5;241m=\u001b[39mhttp_options,\n\u001b[0;32m    279\u001b[0m )\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aio \u001b[38;5;241m=\u001b[39m AsyncClient(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models \u001b[38;5;241m=\u001b[39m Models(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client)\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\google\\genai\\client.py:318\u001b[0m, in \u001b[0;36mClient._get_api_client\u001b[1;34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_config \u001b[38;5;129;01mand\u001b[39;00m debug_config\u001b[38;5;241m.\u001b[39mclient_mode \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplay\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    305\u001b[0m ]:\n\u001b[0;32m    306\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ReplayApiClient(\n\u001b[0;32m    307\u001b[0m       mode\u001b[38;5;241m=\u001b[39mdebug_config\u001b[38;5;241m.\u001b[39mclient_mode,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    308\u001b[0m       replay_id\u001b[38;5;241m=\u001b[39mdebug_config\u001b[38;5;241m.\u001b[39mreplay_id,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m       http_options\u001b[38;5;241m=\u001b[39mhttp_options,\n\u001b[0;32m    316\u001b[0m   )\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseApiClient(\n\u001b[0;32m    319\u001b[0m     vertexai\u001b[38;5;241m=\u001b[39mvertexai,\n\u001b[0;32m    320\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    321\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    322\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[0;32m    323\u001b[0m     location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[0;32m    324\u001b[0m     http_options\u001b[38;5;241m=\u001b[39mhttp_options,\n\u001b[0;32m    325\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py:683\u001b[0m, in \u001b[0;36mBaseApiClient.__init__\u001b[1;34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Implicit initialization or missing arguments.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key:\n\u001b[1;32m--> 683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key inputs argument! To use the Google AI API,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m provide (`api_key`) arguments. To use the Google Cloud API,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m provide (`vertexai`, `project` & `location`) arguments.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    687\u001b[0m     )\n\u001b[0;32m    688\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_options\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://generativelanguage.googleapis.com/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_options\u001b[38;5;241m.\u001b[39mapi_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv1beta\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "api_key_env = os.getenv(\"GOOGLE_API_KEY\")     # from local environment variable, or can use .env file\n",
    "client = genai.Client(api_key=api_key_env)\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "THROTTLE = 1\n",
    "\n",
    "def summarize_article(title: str, summary: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI biotech assistant. Summarize this article in 3 bullet points.\n",
    "    Extract: \n",
    "    1. Main finding\n",
    "    2. Key biological targets (genes, proteins, pathways)\n",
    "    3. Application area (diagnostics, therapeutics, biotech tools, etc.)\n",
    "\n",
    "    Title: {title}\n",
    "    Summary: {summary}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    time.sleep(THROTTLE)         # API Rate limiting\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"raw_summary\": summary,\n",
    "        \"ai_summary\": response.text\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "ai_summary_sample = summarize_article(\n",
    "    \"Updated Full-Text Search Now Available\",\n",
    "    \"As previously announced, NCBI has updated the PubMed Central (PMC) full-text search functionality and user experience...\"\n",
    ")\n",
    "\n",
    "print(ai_summary_sample)\n",
    "\n",
    "# Generate AI summaries for the first 5 articles only\n",
    "articles.loc[:4, \"ai_summary\"] = articles.loc[:4].apply(\n",
    "    lambda row: summarize_article(row[\"title\"], row[\"summary_text\"])[\"ai_summary\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check results\n",
    "print(articles.loc[:4, [\"title\", \"ai_summary\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a082c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 37 articles to data\\rss_summarized.json\n"
     ]
    }
   ],
   "source": [
    "# JSON\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Convert entire DataFrame to list of dicts\n",
    "articles_list = articles.to_dict(orient=\"records\")\n",
    "\n",
    "output_path = Path(\"data/rss_summarized.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles_list, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(articles_list)} articles to {output_path}\")\n",
    "\n",
    "# Add section to append new articles without duplicates - they'll be stored for access by trend agent later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b22ad",
   "metadata": {},
   "source": [
    "## Trend Analysis (Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab29587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(summary_text: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "    Extract keywords and topics from this biotech article summary.\n",
    "    Return as JSON with:\n",
    "    - scientific_terms: genes, proteins, pathways\n",
    "    - companies: biotech companies mentioned\n",
    "    - concepts: biotech concepts or areas (single-cell, AI-drug discovery)\n",
    "    - methods: experimental methods (CRISPR, nanopore sequencing)\n",
    "    \n",
    "    Summary: {summary_text}\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(model=MODEL_NAME, contents=prompt)\n",
    "    # Convert response to dict\n",
    "    import json\n",
    "    try:\n",
    "        topics = json.loads(response.text)\n",
    "    except:\n",
    "        topics = {\"scientific_terms\": [], \"companies\": [], \"concepts\": [], \"methods\": []}\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e0a9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_dict = {\n",
    "    \"single-cell\": [\"scRNA-seq\", \"single cell sequencing\", \"single-cell RNA seq\"],\n",
    "    \"AI-biotech\": [\"AI\", \"machine learning\", \"deep learning\"],\n",
    "    \"biotech IP/legal\": [\"Illumina lawsuit\", \"NGS patents\"]\n",
    "}\n",
    "\n",
    "def normalize_topic(term: str):\n",
    "    for norm, variants in normalization_dict.items():\n",
    "        if term.lower() in [v.lower() for v in variants] or term.lower() == norm.lower():\n",
    "            return norm\n",
    "    return term  # return as-is if no match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518dfbf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ai_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ai_summary'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_normalized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai_summary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m s: {k: [normalize_topic(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m v] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m extract_topics(s)\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      3\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ai_summary'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-8' coro=<BaseApiClient.aclose() done, defined at c:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py:1902> exception=AttributeError(\"'BaseApiClient' object has no attribute '_async_httpx_client'\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1907, in aclose\n",
      "    await self._async_httpx_client.aclose()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BaseApiClient' object has no attribute '_async_httpx_client'\n"
     ]
    }
   ],
   "source": [
    "articles['topics_normalized'] = articles['ai_summary'].apply(\n",
    "    lambda s: {k: [normalize_topic(t) for t in v] for k, v in extract_topics(s).items()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09664ca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'topics_normalized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'topics_normalized'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m trend_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m articles\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      3\u001b[0m     trend_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublished\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_normalized\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclusters\u001b[39m\u001b[38;5;124m\"\u001b[39m: [term_to_cluster[t] \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_normalized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_normalized\u001b[39m\u001b[38;5;124m'\u001b[39m][cat]]\n\u001b[0;32m      9\u001b[0m     })\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/trend_topics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(trend_data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\jeelf\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'topics_normalized'"
     ]
    }
   ],
   "source": [
    "trend_data = []\n",
    "for idx, row in articles.iterrows():\n",
    "    trend_data.append({\n",
    "        \"title\": row['title'],\n",
    "        \"source\": row['source'],\n",
    "        \"published\": row['published'],\n",
    "        \"topics\": row['topics_normalized'],\n",
    "        \"clusters\": [term_to_cluster[t] for cat in row['topics_normalized'] for t in row['topics_normalized'][cat]]\n",
    "    })\n",
    "\n",
    "with open(\"data/trend_topics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(trend_data, f, indent=2, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
